{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Classification Notebook\n",
        "### Deep Learning project - Task #1\n",
        "*Sebastiano Chiari - Francesco Ferrini - Wamiq Raza*"
      ],
      "metadata": {
        "id": "m7V5HI20oV50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup\n",
        "\n",
        "Import section"
      ],
      "metadata": {
        "id": "4VdBZybUogst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as tt\n",
        "from torchvision.io import read_image\n",
        "from torchvision.utils import make_grid\n",
        "from torchsummary import summary\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from skimage import io, transform\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "zASPo3VQofj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Global variables definition"
      ],
      "metadata": {
        "id": "UCRCuS9gvkaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH = '/content/dataset/'\n",
        "MODEL_PATH = '/content/classification_model.pt'\n",
        "\n",
        "ANNOTATIONS_CSV_PATH = DATASET_PATH + 'annotations_train.csv'\n",
        "\n",
        "TRAIN_FOLDER = DATASET_PATH + 'train/'\n",
        "TEST_FOLDER = DATASET_PATH + 'test/'\n",
        "\n",
        "TRAIN_CSV_PATH = DATASET_PATH + 'train_annotation.csv'\n",
        "VALIDATION_CSV_PATH = DATASET_PATH + 'validation_annotations.csv'\n",
        "TEST_CSV_PATH = DATASET_PATH + 'tests.csv'"
      ],
      "metadata": {
        "id": "7eGpEHkOvmxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_PERCENTAGE = 70\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100"
      ],
      "metadata": {
        "id": "NFEBFAYjxjFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Device"
      ],
      "metadata": {
        "id": "ZhJMKD4Iyoxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "id": "aBFEcA8cyoU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "\n",
        "## Dataset\n",
        "\n",
        "Import the dataset and extract the zip file"
      ],
      "metadata": {
        "id": "UWVNVYFHvtdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!unzip \"/content/drive/MyDrive/dataset.zip\" -d dataset"
      ],
      "metadata": {
        "id": "DK_6LaWjvpda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing (create two new csv files, one for training images and the other for validation images)"
      ],
      "metadata": {
        "id": "KdulHd86xvxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(annotations_path=ANNOTATIONS_CSV_PATH, train_path=TRAIN_FOLDER):\n",
        "  # load annotations CSV and retrieve IDs\n",
        "  annotation_csv = pd.read_csv(annotations_path)\n",
        "  \n",
        "  # retrieve all the images from the train folder\n",
        "  train_imgs = os.listdir(train_path)\n",
        "\n",
        "  # get the image id from the file name\n",
        "  train_imgs_data = []\n",
        "  for s in train_imgs:\n",
        "    train_imgs_data.append([int(s.split('_')[0]), s])\n",
        "\n",
        "  # dataframe for image id and filename\n",
        "  train_imgs_df = pd.DataFrame(train_imgs_data, columns=['id', 'filename'])\n",
        "  # merge the 2 dataframes on id value so that we have 1 line for each image name instead that 1 line for each id\n",
        "  annotations_train = pd.merge(annotation_csv, train_imgs_df, on='id')\n",
        "  #Â 1-hot encoding except for age value that has 4 different values [0,1,2,3]\n",
        "  annotations_train.iloc[annotations_train.index,1:28] = annotations_train.iloc[annotations_train.index,1:28]-1\n",
        "  # move the last column in second position\n",
        "  column_to_move = annotations_train.pop(\"filename\")\n",
        "  # move filename column in second position\n",
        "  annotations_train.insert(1, \"filename\", column_to_move)\n",
        "\n",
        "  # get and split IDs\n",
        "  ids = annotation_csv.iloc[:,0].to_numpy()\n",
        "  np.random.shuffle(ids)\n",
        "  val_samples = len(ids) - int((TRAIN_PERCENTAGE / 100) * (len(ids)))\n",
        "  train_ids = ids[:-val_samples]\n",
        "  val_ids = ids[-val_samples:]\n",
        "\n",
        "  train_df = annotations_train.loc[annotations_train['id'].isin(train_ids)]\n",
        "  train_df = train_df.iloc[:, 1:] \n",
        "\n",
        "  val_df = annotations_train.loc[annotations_train['id'].isin(val_ids)]\n",
        "  val_df = val_df.iloc[:, 1:]\n",
        "\n",
        "  # export CSV files\n",
        "  pd.DataFrame(train_df).to_csv(TRAIN_CSV_PATH)\n",
        "  pd.DataFrame(val_df).to_csv(VALIDATION_CSV_PATH)"
      ],
      "metadata": {
        "id": "BScc9TKE2ekO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train dataset class"
      ],
      "metadata": {
        "id": "RaA1hHxYzP87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MarketDataset(Dataset):\n",
        "  def __init__(self, csv_file, root_dir, transform=None):\n",
        "    self.annotations_frame = pd.read_csv(csv_file)\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.annotations_frame)\n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "    \n",
        "    img_path = os.path.join(self.root_dir, str(self.annotations_frame.iloc[idx, 1]))\n",
        "    image = io.imread(img_path)\n",
        "    \n",
        "    sample = {\n",
        "        'image': image,\n",
        "        'annotations': {\n",
        "            'age': np.array([self.annotations_frame.iloc[idx, 2]]).astype('float'),\n",
        "            'backpack': np.array([self.annotations_frame.iloc[idx, 3]]).astype('float'),\n",
        "            'bag': np.array([self.annotations_frame.iloc[idx, 4]]).astype('float'),\n",
        "            'handbag': np.array([self.annotations_frame.iloc[idx, 5]]).astype('float'),\n",
        "            'clothes': np.array([self.annotations_frame.iloc[idx, 6]]).astype('float'),\n",
        "            'down': np.array([self.annotations_frame.iloc[idx, 7]]).astype('float'),\n",
        "            'up': np.array([self.annotations_frame.iloc[idx, 8]]).astype('float'),\n",
        "            'hair': np.array([self.annotations_frame.iloc[idx, 9]]).astype('float'),\n",
        "            'hat': np.array([self.annotations_frame.iloc[idx, 10]]).astype('float'),\n",
        "            'gender': np.array([self.annotations_frame.iloc[idx, 11]]).astype('float'),\n",
        "            'upblack': np.array([self.annotations_frame.iloc[idx, 12]]).astype('float'),\n",
        "            'upwhite': np.array([self.annotations_frame.iloc[idx, 13]]).astype('float'),\n",
        "            'upred': np.array([self.annotations_frame.iloc[idx, 14]]).astype('float'),\n",
        "            'uppurple': np.array([self.annotations_frame.iloc[idx, 15]]).astype('float'),\n",
        "            'upyellow': np.array([self.annotations_frame.iloc[idx, 16]]).astype('float'),\n",
        "            'upgray': np.array([self.annotations_frame.iloc[idx, 17]]).astype('float'),\n",
        "            'upblue': np.array([self.annotations_frame.iloc[idx, 18]]).astype('float'),\n",
        "            'upgreen': np.array([self.annotations_frame.iloc[idx, 19]]).astype('float'),\n",
        "            'downblack': np.array([self.annotations_frame.iloc[idx, 20]]).astype('float'),\n",
        "            'downwhite': np.array([self.annotations_frame.iloc[idx, 21]]).astype('float'),\n",
        "            'downpink': np.array([self.annotations_frame.iloc[idx, 22]]).astype('float'),\n",
        "            'downpurple': np.array([self.annotations_frame.iloc[idx, 23]]).astype('float'),\n",
        "            'downyellow': np.array([self.annotations_frame.iloc[idx, 24]]).astype('float'),\n",
        "            'downgray': np.array([self.annotations_frame.iloc[idx, 25]]).astype('float'),\n",
        "            'downblue': np.array([self.annotations_frame.iloc[idx, 26]]).astype('float'),\n",
        "            'downgreen': np.array([self.annotations_frame.iloc[idx, 27]]).astype('float'),\n",
        "            'downbrown': np.array([self.annotations_frame.iloc[idx, 28]]).astype('float'),\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    if self.transform:\n",
        "      sample['image'] = self.transform(sample['image'])\n",
        "    \n",
        "    return sample"
      ],
      "metadata": {
        "id": "gaEm4vmQzTzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data augmentation"
      ],
      "metadata": {
        "id": "kAXZyajvz30m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imagenet_stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "\n",
        "image_transform = tt.Compose([\n",
        "    tt.ToPILImage(),\n",
        "    tt.RandomCrop((128, 64), padding=8, padding_mode='reflect'),\n",
        "    tt.RandomHorizontalFlip(p=0.5), \n",
        "    tt.RandomRotation(10),\n",
        "    tt.ToTensor(), \n",
        "    tt.Normalize(*imagenet_stats,inplace=True), \n",
        "    tt.RandomErasing(p=0.5, inplace=True)\n",
        "])"
      ],
      "metadata": {
        "id": "BtS3ogNLz5DA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloaders"
      ],
      "metadata": {
        "id": "iu6g2kFs0ddg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(train_csv=TRAIN_CSV_PATH, val_csv=VALIDATION_CSV_PATH, train_path=TRAIN_FOLDER, batch_size=BATCH_SIZE):\n",
        "  # load dataset and create CSV files\n",
        "  preprocessing()\n",
        "\n",
        "  # create the two dataset classes\n",
        "  train_dataset = MarketDataset(train_csv, train_path, transform=image_transform)\n",
        "  val_dataset = MarketDataset(val_csv, train_path, transform=image_transform)\n",
        "\n",
        "  # create dataloaders\n",
        "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "  val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size, num_workers=2, pin_memory=True)\n",
        "\n",
        "  return train_dataloader, val_dataloader"
      ],
      "metadata": {
        "id": "dar2YCjZ0ezo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network definition\n",
        "\n",
        "Custom ResNet50 pretrained"
      ],
      "metadata": {
        "id": "sZ1xG7uEx7C7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomResNet50(nn.Module):\n",
        "  def __init__(self, n_age_classes=4, other_classes=2, dropout_p=0.4):\n",
        "    super(CustomResNet50, self).__init__()\n",
        "    \n",
        "    # import pretrained resnet50 backbone\n",
        "    resnet50 = torchvision.models.resnet50(pretrained=True)\n",
        "    self.fc_inputs = resnet50.fc.in_features\n",
        "    layers = list(resnet50.children())[:-1]\n",
        "    \n",
        "    # edit last layer of the network\n",
        "    self.feature_extractor = nn.Sequential(*layers)\n",
        "    \n",
        "    # create separate classifiers for each class\n",
        "    self.age = nn.Sequential(\n",
        "        nn.Dropout(p=dropout_p), \n",
        "        nn.Linear(self.fc_inputs,1000), \n",
        "        nn.ReLU(), nn.Dropout(p=dropout_p), \n",
        "        nn.Linear(1000,n_age_classes)\n",
        "    )\n",
        "    self.bagpack = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(self.fc_inputs,1000), nn.ReLU(), nn.Dropout(p=dropout_p), nn.Linear(1000,other_classes))\n",
        "    self.bag = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(self.fc_inputs,1000), nn.ReLU(), nn.Dropout(p=dropout_p), nn.Linear(1000,other_classes))\n",
        "    self.handbag = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(self.fc_inputs,1000), nn.ReLU(), nn.Dropout(p=dropout_p), nn.Linear(1000,other_classes))\n",
        "    self.clothes = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(self.fc_inputs,1000), nn.ReLU(), nn.Dropout(p=dropout_p), nn.Linear(1000,other_classes))\n",
        "    self.down = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(self.fc_inputs,1000), nn.ReLU(), nn.Dropout(p=dropout_p), nn.Linear(1000,other_classes))\n",
        "    self.up = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(self.fc_inputs,1000), nn.ReLU(), nn.Dropout(p=dropout_p), nn.Linear(1000,other_classes))\n",
        "    self.hair = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(self.fc_inputs,1000), nn.ReLU(), nn.Dropout(p=dropout_p), nn.Linear(1000,other_classes))\n",
        "    self.hat = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(self.fc_inputs,1000), nn.ReLU(), nn.Dropout(p=dropout_p), nn.Linear(1000,other_classes))\n",
        "    self.gender = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(self.fc_inputs,1000), nn.ReLU(), nn.Dropout(p=dropout_p), nn.Linear(1000,other_classes))\n",
        "    self.upblack = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(self.fc_inputs,1000), nn.ReLU(), nn.Dropout(p=dropout_p), nn.Linear(1000,other_classes))\n",
        "    self.upwhite = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(self.fc_inputs,1000), nn.ReLU(), nn.Dropout(p=dropout_p), nn.Linear(1000,other_classes))\n",
        "    self.upred = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(self.fc_inputs,1000), nn.ReLU(), nn.Dropout(p=dropout_p), nn.Linear(1000,other_classes))\n",
        "    self.uppurple = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(self.fc_inputs,1000), nn.ReLU(), nn.Dropout(p=dropout_p), nn.Linear(1000,other_classes))\n",
        "    self.upyellow = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(self.fc_inputs,1000), nn.ReLU(), nn.Dropout(p=dropout_p), nn.Linear(1000,other_classes))\n",
        "    self.upgray = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(self.fc_inputs,1000), nn.ReLU(), nn.Dropout(p=dropout_p), nn.Linear(1000,other_classes))\n",
        "    self.upblue = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(self.fc_inputs,1000), nn.ReLU(), nn.Dropout(p=dropout_p), nn.Linear(1000,other_classes))\n",
        "    self.upgreen = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(self.fc_inputs,1000), nn.ReLU(), nn.Dropout(p=dropout_p), nn.Linear(1000,other_classes))\n",
        "    self.downblack = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(self.fc_inputs,1000), nn.ReLU(), nn.Dropout(p=dropout_p), nn.Linear(1000,other_classes))\n",
        "    self.downwhite = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(self.fc_inputs,1000), nn.ReLU(), nn.Dropout(p=dropout_p), nn.Linear(1000,other_classes))\n",
        "    self.downpink = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(self.fc_inputs,1000), nn.ReLU(), nn.Dropout(p=dropout_p), nn.Linear(1000,other_classes))\n",
        "    self.downpurple = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(self.fc_inputs,1000), nn.ReLU(), nn.Dropout(p=dropout_p), nn.Linear(1000,other_classes))\n",
        "    self.downyellow = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(self.fc_inputs,1000), nn.ReLU(), nn.Dropout(p=dropout_p), nn.Linear(1000,other_classes))\n",
        "    self.downgray = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(self.fc_inputs,1000), nn.ReLU(), nn.Dropout(p=dropout_p), nn.Linear(1000,other_classes))\n",
        "    self.downblue = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(self.fc_inputs,1000), nn.ReLU(), nn.Dropout(p=dropout_p), nn.Linear(1000,other_classes))\n",
        "    self.downgreen = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(self.fc_inputs,1000), nn.ReLU(), nn.Dropout(p=dropout_p), nn.Linear(1000,other_classes))\n",
        "    self.downbrown = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(self.fc_inputs,1000), nn.ReLU(), nn.Dropout(p=dropout_p), nn.Linear(1000,other_classes))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.feature_extractor(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    return {\n",
        "        'age': self.age(x),\n",
        "        'backpack': self.bagpack(x),\n",
        "        'bag': self.bag(x),\n",
        "        'handbag': self.handbag(x),\n",
        "        'clothes': self.clothes(x),\n",
        "        'down': self.down(x),\n",
        "        'up': self.up(x),\n",
        "        'hair': self.hair(x),\n",
        "        'hat': self.hat(x),\n",
        "        'gender': self.gender(x),\n",
        "        'upblack': self.upblack(x),\n",
        "        'upwhite': self.upwhite(x),\n",
        "        'upred': self.upred(x),\n",
        "        'uppurple': self.uppurple(x),\n",
        "        'upyellow': self.upyellow(x),\n",
        "        'upgray': self.upgray(x),\n",
        "        'upblue': self.upblue(x),\n",
        "        'upgreen': self.upgreen(x),\n",
        "        'downblack': self.downblack(x),\n",
        "        'downwhite': self.downwhite(x),\n",
        "        'downpink': self.downpink(x),\n",
        "        'downpurple': self.downpurple(x),\n",
        "        'downyellow': self.downyellow(x),\n",
        "        'downgray': self.downgray(x),\n",
        "        'downblue': self.downblue(x),\n",
        "        'downgreen': self.downgreen(x),\n",
        "        'downbrown': self.downbrown(x)\n",
        "    }"
      ],
      "metadata": {
        "id": "TQc9sCEXx7j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Early Stopping"
      ],
      "metadata": {
        "id": "EDsYKDT46KFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path=MODEL_PATH, trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model, self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "metadata": {
        "id": "Y9_s-lw66MUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss / cost function"
      ],
      "metadata": {
        "id": "Tjax1JUO1cSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cost_function():\n",
        "  return nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "0UagVAxJ1fTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy"
      ],
      "metadata": {
        "id": "km-qO7wu_aix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Function to get both the total loss summing all the losses and the accuracies:\n",
        "  - loss: is the total loss\n",
        "  - acc: dictionary with all the accuracies\n",
        "'''\n",
        "\n",
        "def get_loss_and_accuracies(truth_annotations, outputs, cost_func, device):\n",
        "  loss = 0.0\n",
        "  acc = {}\n",
        "  for key, value in truth_annotations.items():\n",
        "    loss += cost_func(outputs[key], value.type(torch.LongTensor).squeeze(1).to(device)) # calculate the loss\n",
        "    acc[key] = accuracy(value, outputs[key])\n",
        "  return loss, acc\n",
        "\n",
        "def accuracy(truth_labels, outputs):\n",
        "  _, predicted = torch.max(outputs, dim=1)\n",
        "  return predicted.eq(truth_labels.squeeze(1)).sum().item() # Note: the .item() is needed to extract scalars from tensors\n",
        "\n",
        "def calculate_total_accuracy(total_accuracy, current_accuracies):\n",
        "  if len(total_accuracy) == 0:\n",
        "    total_accuracy = current_accuracies\n",
        "    return total_accuracy\n",
        "\n",
        "  for key, value in total_accuracy.items():\n",
        "    total_accuracy[key] += current_accuracies[key]\n",
        "  return total_accuracy"
      ],
      "metadata": {
        "id": "SsGenm2I_aPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer"
      ],
      "metadata": {
        "id": "yoUGmESo1ftO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer(net, lr, wd, momentum):\n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr, wd, momentum)\n",
        "  # optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=wd)\n",
        "  return optimizer"
      ],
      "metadata": {
        "id": "HKb0LhBa1go4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adaptive Learning Rate"
      ],
      "metadata": {
        "id": "aw6A_EMSp2lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_scheduler(optimizer):\n",
        "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
        "  return scheduler"
      ],
      "metadata": {
        "id": "OdxJrGIxntde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train step"
      ],
      "metadata": {
        "id": "DRjcHVZt1huk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_model(model, batch_size, data_loader, cost_func, optimizer, device, n_outputs=27):\n",
        "  model.train()\n",
        "\n",
        "  # loss\n",
        "  total_train_loss = 0.0\n",
        "\n",
        "  # total accuracy\n",
        "  total_accuracy = {}\n",
        "\n",
        "  batch_counter = 0\n",
        "  tot_samples = 0\n",
        "\n",
        "  for batch_idx, input_data in enumerate(tqdm(data_loader, desc=\"Training\", leave=False)): #enumerate(data_loader):\n",
        "    # push input data to GPU\n",
        "    input_img = input_data['image'].to(device)\n",
        "    # move annotations (dict) to device\n",
        "    truth_annotations = input_data['annotations']\n",
        "    for key, value in truth_annotations.items():\n",
        "      truth_annotations[key] = truth_annotations[key].to(device)\n",
        "    # get outputs (predictions) from the model\n",
        "    outputs = model(input_img.float())\n",
        "\n",
        "    # push output to GPU\n",
        "    for key, value in outputs.items():\n",
        "      outputs[key] = outputs[key].to(device)\n",
        "    #outputs = move_to(outputs, device)\n",
        "\n",
        "    # calculate loss, summ all classifiers losses\n",
        "    loss, current_accuracies = get_loss_and_accuracies(truth_annotations, outputs, cost_func, device)\n",
        "\n",
        "    # backward\n",
        "    loss.backward()\n",
        "\n",
        "    # update weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # reset gradiants to zero\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # update total training loss\n",
        "    total_train_loss += loss/n_outputs\n",
        "\n",
        "    # get accuracies\n",
        "    #current_accuracies = get_all_classifiers_accuracies(truth_annotations, outputs)\n",
        "    total_accuracy = calculate_total_accuracy(total_accuracy, current_accuracies)\n",
        "\n",
        "    tot_samples += input_img.shape[0] # batch size\n",
        "    batch_counter += 1\n",
        "\n",
        "  # normalize accuracy and loss\n",
        "  for key, value in total_accuracy.items():\n",
        "    total_accuracy[key] = value / tot_samples*100\n",
        "\n",
        "  batch_loss = total_train_loss/batch_counter\n",
        "\n",
        "  return batch_loss, total_accuracy"
      ],
      "metadata": {
        "id": "gWyXXCt11jQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation step"
      ],
      "metadata": {
        "id": "gvsPIq-I_1Bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_model(model, batch_size, data_loader, cost_func, device, n_outputs=27):\n",
        "  model.eval()\n",
        "\n",
        "  # loss\n",
        "  total_valid_loss = 0.0\n",
        "\n",
        "  # total accuracy\n",
        "  total_accuracy = {}\n",
        "\n",
        "  batch_counter = 0\n",
        "  tot_samples = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, input_data in enumerate(tqdm(data_loader, desc=\"Validation\", leave=False)): # enumerate(data_loader):\n",
        "      # push input data to GPU\n",
        "      input_img = input_data['image'].to(device)\n",
        "      # move annotations (dict) to device\n",
        "      truth_annotations = input_data['annotations']\n",
        "      for key, value in truth_annotations.items():\n",
        "        truth_annotations[key] = truth_annotations[key].to(device)\n",
        "\n",
        "      # get outputs (predictions) from the model\n",
        "      outputs = model(input_img.float())\n",
        "\n",
        "      # push outputs to GPU\n",
        "      for key, value in outputs.items():\n",
        "        outputs[key] = outputs[key].to(device)\n",
        "\n",
        "      # calculate loss, summ all classifiers losses\n",
        "      loss, current_accuracies = get_loss_and_accuracies(truth_annotations, outputs, cost_func, device)\n",
        "\n",
        "      # update total training loss\n",
        "      total_valid_loss += loss/n_outputs\n",
        "\n",
        "      # get accurecies\n",
        "      #current_accuracies = get_all_classifiers_accuracies(truth_annotations, outputs)\n",
        "      total_accuracy = calculate_total_accuracy(total_accuracy, current_accuracies)\n",
        "\n",
        "      tot_samples += input_img.shape[0] # batch elem size\n",
        "      batch_counter += 1\n",
        "\n",
        "    # normalize accuracy and loss\n",
        "    for key, value in total_accuracy.items():\n",
        "      total_accuracy[key] = value / tot_samples*100\n",
        "\n",
        "    batch_loss = total_valid_loss/batch_counter\n",
        "\n",
        "    return batch_loss, total_accuracy"
      ],
      "metadata": {
        "id": "L8erp7Hl_ybC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "CszpztRu_6UB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_values(writer, step, loss, accuracy, prefix):\n",
        "  writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
        "  writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)"
      ],
      "metadata": {
        "id": "TrOvG7jrNN-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(batch_size=64,\n",
        "         device='cuda:0',\n",
        "         learning_rate=0.01,\n",
        "         weight_decay=0.01,\n",
        "         momentum=0.9,\n",
        "         epochs=100,\n",
        "         ):\n",
        "  \n",
        "  writer = SummaryWriter(log_dir=\"runs/exp\")\n",
        "\n",
        "  # instantiate dataloaders\n",
        "  train_loader, valid_loader = get_data(TRAIN_CSV_PATH, VALIDATION_CSV_PATH, TRAIN_FOLDER, batch_size)\n",
        "\n",
        "  # instantiate network\n",
        "  net = CustomResNet50().to(device)\n",
        "\n",
        "  # instatiate optimizer\n",
        "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "\n",
        "  # instantiate scheduler for adaptive learning rate\n",
        "  scheduler = get_scheduler(optimizer) \n",
        "\n",
        "  # instantiate cost function\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  # instantiate early stopping\n",
        "  early_stopping = EarlyStopping(verbose=True)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    total_train_acc = 0.0\n",
        "    total_valid_acc = 0.0\n",
        "\n",
        "    print('-' * 220)\n",
        "    print('| epoch {:3d}/{:3d}'.format(epoch +1, epochs))\n",
        "\n",
        "    train_loss, train_accuracy = training_model(net, batch_size, train_loader, cost_function, optimizer, device)\n",
        "\n",
        "    tr = '| train_loss: {:.3f}'.format(train_loss.data)\n",
        "    for key, value in train_accuracy.items():\n",
        "      tr += ' | ' + key + ': {:.3f}'.format(value)\n",
        "      total_train_acc += value\n",
        "    tr += ' | avg_acc: {:.3f}'.format(total_train_acc/len(train_accuracy))\n",
        "    print(tr)\n",
        "\n",
        "    val_loss, val_accuracy = validate_model(net, batch_size, valid_loader, cost_function, device)\n",
        "\n",
        "    acc = '| valid_loss: {:.3f}'.format(val_loss.data)\n",
        "    for key, value in val_accuracy.items():\n",
        "      acc += ' | ' + key + ': {:.3f}'.format(value)\n",
        "      total_valid_acc += value \n",
        "    acc += ' | avg_acc: {:.3f}'.format(total_valid_acc/len(val_accuracy))\n",
        "    print(acc)\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # log to tensorboard\n",
        "    log_values(writer, epoch, train_loss, total_train_acc/len(train_accuracy), \"Train\")\n",
        "    log_values(writer, epoch, val_loss, total_valid_acc/len(val_accuracy), \"Validation\")\n",
        "\n",
        "    early_stopping(val_loss, net)\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "      print(\"Early stopping has occurred. Reverting to latest save model\")\n",
        "      break\n",
        "\n",
        "  # closes the logger\n",
        "  writer.close()"
      ],
      "metadata": {
        "id": "5eXms3sTB16R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "UHRrl63q__I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "A6mz-ZI4_-wQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction Step"
      ],
      "metadata": {
        "id": "R8OxTHmlAPE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preprocessing"
      ],
      "metadata": {
        "id": "QzcI4EHSKb5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_preprocessing(test_folder=TEST_FOLDER, test_csv_path=TEST_CSV_PATH):\n",
        "  # create test dataframe\n",
        "  test_imgs = os.listdir(test_folder)\n",
        "  test_df = pd.DataFrame(test_imgs, columns=['filename'])\n",
        "\n",
        "  # convert dataframe to cs\n",
        "  test_df.to_csv(test_csv_path)"
      ],
      "metadata": {
        "id": "Yu3ZcXtVAQU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test dataset class"
      ],
      "metadata": {
        "id": "DKX8PIgKKjVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestMarketDataset(Dataset):\n",
        "  \"\"\" Market test dataset \"\"\"\n",
        "\n",
        "  def __init__(self, csv_file, root_dir, transform=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.annotations_frame = pd.read_csv(csv_file)\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.annotations_frame)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "        idx = idx.tolist()\n",
        "\n",
        "    img_path = os.path.join(self.root_dir, str(self.annotations_frame.iloc[idx, 1]))\n",
        "    image = io.imread(img_path)\n",
        "\n",
        "    sample = {\n",
        "        'image': image,\n",
        "        'image_filename': self.annotations_frame.iloc[idx, 1],\n",
        "        'image_path': img_path\n",
        "    }\n",
        "\n",
        "    if self.transform:\n",
        "        sample['image'] = self.transform(sample['image'])\n",
        "\n",
        "    return sample"
      ],
      "metadata": {
        "id": "QQcedRhnKkJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data augmentation"
      ],
      "metadata": {
        "id": "qu6pSpdoKkf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imagenet_stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "\n",
        "test_transform = tt.Compose([\n",
        "    tt.ToTensor(), \n",
        "    tt.Normalize(*imagenet_stats,inplace=True)\n",
        "])"
      ],
      "metadata": {
        "id": "MWJg3tJoKlKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataloader"
      ],
      "metadata": {
        "id": "19yPERAILCfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_data(test_csv=TEST_CSV_PATH, test_folder=TEST_FOLDER, transform=test_transform):\n",
        "  # preprocess data\n",
        "  test_preprocessing(test_folder, test_csv)\n",
        "  \n",
        "  # build dataset object\n",
        "  test_dataset = TestMarketDataset(test_csv, test_folder, transform)\n",
        "  \n",
        "  # instantiate dataloader\n",
        "  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, num_workers=2, pin_memory=True)\n",
        "\n",
        "  return test_loader"
      ],
      "metadata": {
        "id": "aP7FENIGLDOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test function"
      ],
      "metadata": {
        "id": "VPIzK5jzLdRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prediction(img_id, outputs):\n",
        "  result = []\n",
        "  result.append(img_id)\n",
        "  for key, values in outputs.items():\n",
        "    _, pred = torch.max(values, dim=1)\n",
        "    result.append(int(pred.cpu().detach().numpy()))\n",
        "  return result"
      ],
      "metadata": {
        "id": "klcwYdnBLeGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, data_loader, device):\n",
        "  model.eval()\n",
        "\n",
        "  # pedictions\n",
        "  predictions = []\n",
        "\n",
        "  print('Testing has started ..')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, input_data in enumerate(tqdm(data_loader)):\n",
        "\n",
        "      # push input data to GPU\n",
        "      input_img = input_data['image'].to(device)\n",
        "      input_img_id = str(input_data['image_filename'][0])\n",
        "\n",
        "      # get outputs (predictions) from the model\n",
        "      outputs = model(input_img.float())\n",
        "\n",
        "      # push output to GPU\n",
        "      for key, value in outputs.items():\n",
        "        outputs[key] = outputs[key].to(device)\n",
        "\n",
        "      # get final predictions\n",
        "      final_result = get_prediction(input_img_id, outputs)\n",
        "\n",
        "      predictions.append(final_result)\n",
        "    \n",
        "    print('Testing has finished!!')\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "LauHjtOULuI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Export predictions"
      ],
      "metadata": {
        "id": "JI5G2gZFL7o2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def export_predictions(predictions_csv, model_path=MODEL_PATH, device='cuda:0'):\n",
        "  # get data\n",
        "  test_loader = get_test_data()\n",
        "\n",
        "  # load model\n",
        "  model = torch.load(MODEL_PATH)\n",
        "\n",
        "  # run test model on test images\n",
        "  predictions = test_model(model, test_loader, device)\n",
        "  \n",
        "  # export classifier outcome to dataframe\n",
        "  predictions_df = pd.DataFrame(predictions, columns=['filename', 'age', 'backpack', 'bag','handbag','clothes','down','up','hair','hat','gender', 'upblack','upwhite','upred','uppurple','upyellow','upgray','upblue','upgreen','downblack','downwhite','downpink','downpurple','downyellow','downgray','downblue','downgreen','downbrown'])\n",
        "  \n",
        "  # save final predictions to csv\n",
        "  predictions_df.to_csv(predictions_csv, index=False)"
      ],
      "metadata": {
        "id": "-2GYF9IuL8Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_csv_path = DATASET_PATH + 'classification_test.csv'\n",
        "export_predictions(predictions_csv_path)"
      ],
      "metadata": {
        "id": "uWqX4W_ONy2N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}